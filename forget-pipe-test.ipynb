{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-12T03:04:18.340741Z",
     "start_time": "2024-04-12T03:04:16.949972Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T03:04:18.369276Z",
     "start_time": "2024-04-12T03:04:18.365628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "if \"forget-from-image\" not in cwd:\n",
    "    os.chdir(\"forget-from-image\")\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '6,7'\n",
    "device1 = torch.device(\"cuda:6\")\n",
    "device2 = torch.device(\"cuda:7\")"
   ],
   "id": "81e7aa104f289dbe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T03:04:21.211374Z",
     "start_time": "2024-04-12T03:04:19.924336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from prompt2concept.ner_utils import StyleExtractor\n",
    "from img2prompt.imgConvertor import ImgConvertor"
   ],
   "id": "660301764e2d602b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/yifei/anaconda3/envs/py3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-12T03:04:21.785259Z"
    }
   },
   "cell_type": "code",
   "source": "imgConvertor = ImgConvertor(device = device1)",
   "id": "9c79cf5c9ebd3871",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP model...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "styleExtractor = StyleExtractor(device = device2)",
   "id": "3fa093ecd6805847",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T03:03:06.717403Z",
     "start_time": "2024-04-12T03:03:05.926074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imgPath = \"examples/monet.jpg\"\n",
    "raw_prompt = imgConvertor.inference(imgPath,\"fast\")\n",
    "print(raw_prompt)"
   ],
   "id": "e4fd268b480585e7",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['encoder_hidden_states', 'encoder_attention_mask'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m imgPath \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexamples/monet.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m raw_prompt \u001B[38;5;241m=\u001B[39m \u001B[43mimgConvertor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgPath\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfast\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(raw_prompt)\n",
      "File \u001B[0;32m~/forget-from-image/img2prompt/imgConvertor.py:33\u001B[0m, in \u001B[0;36mImgConvertor.inference\u001B[0;34m(self, image_path, mode, best_max_flavors)\u001B[0m\n\u001B[1;32m     31\u001B[0m     prompt_result \u001B[38;5;241m=\u001B[39m ci\u001B[38;5;241m.\u001B[39minterrogate_classic(image)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# 'fast'\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m     prompt_result \u001B[38;5;241m=\u001B[39m \u001B[43mci\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterrogate_fast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# print(f\"Mode {mode}: {prompt_result}\")\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m prompt_result\n",
      "File \u001B[0;32m~/forget-from-image/img2prompt/clip-interrogator/clip_interrogator/clip_interrogator.py:162\u001B[0m, in \u001B[0;36mInterrogator.interrogate_fast\u001B[0;34m(self, image, max_flavors)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minterrogate_fast\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: Image, max_flavors: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m32\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m--> 162\u001B[0m     caption \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_caption\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m     image_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_to_features(image)\n\u001B[1;32m    164\u001B[0m     merged \u001B[38;5;241m=\u001B[39m _merge_tables([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39martists, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflavors, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmediums, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmovements, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrendings], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig)\n",
      "File \u001B[0;32m~/forget-from-image/img2prompt/clip-interrogator/clip_interrogator/clip_interrogator.py:126\u001B[0m, in \u001B[0;36mInterrogator.generate_caption\u001B[0;34m(self, pil_image)\u001B[0m\n\u001B[1;32m    119\u001B[0m gpu_image \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[1;32m    120\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((size, size), interpolation\u001B[38;5;241m=\u001B[39mInterpolationMode\u001B[38;5;241m.\u001B[39mBICUBIC),\n\u001B[1;32m    121\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[1;32m    122\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mNormalize((\u001B[38;5;241m0.48145466\u001B[39m, \u001B[38;5;241m0.4578275\u001B[39m, \u001B[38;5;241m0.40821073\u001B[39m), (\u001B[38;5;241m0.26862954\u001B[39m, \u001B[38;5;241m0.26130258\u001B[39m, \u001B[38;5;241m0.27577711\u001B[39m))\n\u001B[1;32m    123\u001B[0m ])(pil_image)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 126\u001B[0m     caption \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblip_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgpu_image\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    128\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    129\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblip_num_beams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    130\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblip_max_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    131\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\n\u001B[1;32m    132\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mblip_offload:\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblip_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblip_model\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/forget-from-image/img2prompt/src/blip/models/blip.py:156\u001B[0m, in \u001B[0;36mBLIP_Decoder.generate\u001B[0;34m(self, image, sample, num_beams, max_length, min_length, top_p, repetition_penalty)\u001B[0m\n\u001B[1;32m    144\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_decoder\u001B[38;5;241m.\u001B[39mgenerate(input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    145\u001B[0m                                           max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m    146\u001B[0m                                           min_length\u001B[38;5;241m=\u001B[39mmin_length,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    152\u001B[0m                                           repetition_penalty\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.1\u001B[39m,                                            \n\u001B[1;32m    153\u001B[0m                                           \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;66;03m#beam search\u001B[39;00m\n\u001B[0;32m--> 156\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mmin_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmin_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_beams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msep_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m     \u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mrepetition_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepetition_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m                                          \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m            \n\u001B[1;32m    165\u001B[0m captions \u001B[38;5;241m=\u001B[39m []    \n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs:\n",
      "File \u001B[0;32m~/anaconda3/envs/py3.8/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/py3.8/lib/python3.8/site-packages/transformers/generation_utils.py:1146\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001B[0m\n\u001B[1;32m    911\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    912\u001B[0m \n\u001B[1;32m    913\u001B[0m \u001B[38;5;124;03mGenerates sequences of token ids for models with a language modeling head. The method supports the following\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1143\u001B[0m \u001B[38;5;124;03m['Paris ist eines der dichtesten besiedelten Gebiete Europas.']\u001B[39;00m\n\u001B[1;32m   1144\u001B[0m \u001B[38;5;124;03m```\"\"\"\u001B[39;00m\n\u001B[1;32m   1145\u001B[0m \u001B[38;5;66;03m# 0. Validate model kwargs\u001B[39;00m\n\u001B[0;32m-> 1146\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_model_kwargs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1148\u001B[0m \u001B[38;5;66;03m# 1. Set generation parameters if not already defined\u001B[39;00m\n\u001B[1;32m   1149\u001B[0m bos_token_id \u001B[38;5;241m=\u001B[39m bos_token_id \u001B[38;5;28;01mif\u001B[39;00m bos_token_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mbos_token_id\n",
      "File \u001B[0;32m~/anaconda3/envs/py3.8/lib/python3.8/site-packages/transformers/generation_utils.py:861\u001B[0m, in \u001B[0;36mGenerationMixin._validate_model_kwargs\u001B[0;34m(self, model_kwargs)\u001B[0m\n\u001B[1;32m    858\u001B[0m         unused_model_args\u001B[38;5;241m.\u001B[39mappend(key)\n\u001B[1;32m    860\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m unused_model_args:\n\u001B[0;32m--> 861\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    862\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following `model_kwargs` are not used by the model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00munused_model_args\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (note: typos in the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    863\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m generate arguments will also show up in this list)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    864\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: The following `model_kwargs` are not used by the model: ['encoder_hidden_states', 'encoder_attention_mask'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
